{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random, math\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = '/home/pc/Desktop_linux/chinu/big_mart_sales_prediction/train_v9rqX0R.csv'\n",
    "unseen_path = '/home/pc/Desktop_linux/chinu/big_mart_sales_prediction/test_AbJTz2l.csv'\n",
    "\n",
    "df = pd.read_csv(df_path)\n",
    "unseen_df = pd.read_csv(unseen_path)\n",
    "print(df.head())\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Item weights DF for each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights = df[['Item_Identifier', 'Item_Weight']].dropna(axis=0)\n",
    "df_weights = df_weights.groupby(by='Item_Identifier')['Item_Weight'].apply(lambda x: x.mode().iloc[0]).reset_index()\n",
    "df_weights.head()\n",
    "\n",
    "df_Item_Identifier = df.groupby(by='Item_Identifier')['Item_Outlet_Sales'].mean().reset_index()\n",
    "# df_weights.head(), df_Item_Identifier.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROCERY_STORE_SIZE = df[df['Outlet_Type'] == 'Grocery Store']['Outlet_Size'].dropna().unique()[0]\n",
    "LOCATION_TIER_2_SIZE = df[df['Outlet_Location_Type'] == 'Tier 2']['Outlet_Size'].dropna().unique()[0]\n",
    "\n",
    "\n",
    "def replace_missing_values(row):\n",
    "    if pd.isna(row['Item_Weight']):\n",
    "        mode_val = df_weights.loc[df_weights['Item_Identifier'] == row['Item_Identifier'], 'Item_Weight'] \n",
    "        if not mode_val.empty:\n",
    "            row['Item_Weight'] = mode_val.iloc[0]\n",
    "        else:\n",
    "            print(row['Item_Identifier'])\n",
    "    \n",
    "    if pd.isna(row['Outlet_Size']):\n",
    "        if row['Outlet_Type'] == 'Grocery Store':\n",
    "            row['Outlet_Size'] = GROCERY_STORE_SIZE\n",
    "        elif row['Outlet_Location_Type'] == 'Tier 2':\n",
    "            row['Outlet_Size'] = LOCATION_TIER_2_SIZE\n",
    "        else:\n",
    "            print('NaN is as it is !!', row['Outlet_Identifier'], row['Outlet_Location_Type'], row['Outlet_Type'])\n",
    "\n",
    "    # assign 1 for low fat else 0\n",
    "    my_dict = {'Low Fat' : 1,\n",
    "               'Regular' : 0,\n",
    "               'LF' : 1,\n",
    "               'reg' : 0,\n",
    "               'low fat' : 1}\n",
    "    row['Item_Fat_Content'] = my_dict[row['Item_Fat_Content']]\n",
    "\n",
    "\n",
    "    # Feature enginnering for establish_year\n",
    "    row['Outlet_Total_Years'] = int(2013 - row['Outlet_Establishment_Year'])\n",
    "\n",
    "    # Handle Visibility feature\n",
    "    row['Item_Vis_Log'] = math.log(row['Item_Visibility'] * 1000 + 1)\n",
    "\n",
    "    # Target Encoding for Item_Identifier\n",
    "    row['Item_Identifier_encoded'] = df_Item_Identifier.loc[df_Item_Identifier['Item_Identifier'] == row['Item_Identifier'], 'Item_Outlet_Sales'].iloc[0]\n",
    "    return row\n",
    "\n",
    "\n",
    "df_clean = df.apply(replace_missing_values, axis=1)\n",
    "unseen_df_clean = unseen_df.apply(replace_missing_values, axis=1)\n",
    "\n",
    "df_clean['Item_Weight'] = df_clean['Item_Weight'].fillna(df_clean['Item_Weight'].mode().iloc[0])\n",
    "unseen_df_clean['Item_Weight'] = unseen_df_clean['Item_Weight'].fillna(df_clean['Item_Weight'].mode().iloc[0])\n",
    "\n",
    "df_clean.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encoding with KFOLd for 'Item_Identifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# df_clean['Item_Identifier_encoded'] = np.nan\n",
    "\n",
    "# for i, (train_index, val_index) in enumerate(kf.split(df_clean)):\n",
    "#     train_df = df_clean.iloc[train_index].copy()  # Create copies to avoid SettingWithCopyWarning\n",
    "#     val_df = df_clean.iloc[val_index].copy()    # Create copies to avoid SettingWithCopyWarning\n",
    "\n",
    "#     means = train_df.groupby('Item_Identifier')['Item_Outlet_Sales'].mean().round(2)\n",
    "\n",
    "#     # The fix: Use the indices from val_df directly\n",
    "#     df_clean.loc[val_df.index, 'Item_Identifier_encoded'] = val_df['Item_Identifier'].map(means)\n",
    "\n",
    "\n",
    "# # Calculate mean for overall df_clean\n",
    "# means = df_clean.groupby(by='Item_Identifier')['Item_Outlet_Sales'].mean().round(2)\n",
    "# df_clean['Item_Identifier_encoded'] = df_clean['Item_Identifier_encoded'].fillna(df_clean['Item_Outlet_Sales'].mean().round(2))\n",
    "# unseen_df_clean['Item_Identifier_encoded'] = unseen_df_clean['Item_Identifier'].map(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding for Item_Type, Outlet_Identifier, Outlet_Size, Outlet_Location_Type, Outlet_Type    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(temp_df):\n",
    "    dummies = pd.get_dummies(temp_df[['Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']], dtype=int)\n",
    "    new_df = pd.concat([temp_df, dummies], axis=1)\n",
    "    new_df = new_df.drop(['Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'Outlet_Establishment_Year'], axis=1)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "df_clean = one_hot_encoding(df_clean)\n",
    "unseen_df_clean = one_hot_encoding(unseen_df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.Figure(figsize=(10, 10))\n",
    "sns.histplot(data=df_clean, x='Item_Vis_Log', bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[df_clean['Item_Vis_Log'] < 1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = [ 'Item_Weight', 'Item_Visibility', 'Item_MRP', 'Item_Identifier_encoded', 'Outlet_Total_Years','Item_Outlet_Sales']\n",
    "plt.Figure(figsize=(10, 10))\n",
    "sns.heatmap(df_clean[num_col].corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train- Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_clean['Item_Outlet_Sales'].iloc[2000:3000]\n",
    "X = df_clean.drop(['Item_Outlet_Sales', 'Item_Identifier', 'Item_Visibility', 'Item_Identifier_encoded'], axis=1).iloc[2000:3000]\n",
    "X_unseen = unseen_df_clean.drop(['Item_Identifier', 'Item_Visibility', 'Item_Identifier_encoded'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape, X_unseen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaling_col = ['Item_Weight', 'Item_Vis_Log', 'Item_MRP', 'Outlet_Total_Years']\n",
    "X_train[scaling_col] = scaler.fit_transform(X_train[scaling_col])\n",
    "X_unseen[scaling_col] = scaler.transform(X_unseen[scaling_col])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(y_train, y_train_pred, y_test, y_test_pred, model='Regression'):\n",
    "    print(\"*\" * 80)\n",
    "    print(\"*\" * 30, model, '*' * 30)\n",
    "    print(\"*\" * 80)\n",
    "    print('Train Data : ')\n",
    "    print('RMSE = ', root_mean_squared_error(y_train, y_train_pred))\n",
    "    print('R2_score = ', r2_score(y_train, y_train_pred))\n",
    "    print('\\n\\nTest Data : ')\n",
    "    print('RMSE = ', root_mean_squared_error(y_test, y_test_pred))\n",
    "    print('R2_score = ', r2_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'n_estimators': [ 50, 70, 100, 150, 200],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'min_samples_split': [5, 10, 15],\n",
    "    'min_samples_leaf': [5, 10, 15],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# 1. Define the RMSE function\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return np.sqrt(mse)\n",
    "\n",
    "# 2. Create the RMSE scorer using make_scorer\n",
    "rmse_scorer = make_scorer(root_mean_squared_error, greater_is_better=False) # Important!\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf,\n",
    "                           scoring=rmse_scorer,  # Use the scorer here\n",
    "                           verbose=2,\n",
    "                           cv=4,\n",
    "                           param_grid=param,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "y_train_pred = best_estimator.predict(X_train)\n",
    "y_test_pred = best_estimator.predict(X_test)\n",
    "y_unseen_pred = best_estimator.predict(X_unseen)\n",
    "model_evaluation(y_train, y_train_pred, y_test, y_test_pred, 'RF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Rgression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LinearRegression()\n",
    "# lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_pred = lr.predict(X_train)\n",
    "# y_test_pred = lr.predict(X_test)\n",
    "# y_unseen_pred = lr.predict(X_unseen)\n",
    "# model_evaluation(y_train, y_train_pred, y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param = {'alpha': np.logspace(-3, 3, 7)}\n",
    "\n",
    "# ridge = Ridge()\n",
    "\n",
    "\n",
    "# def root_mean_squared_error(y_true, y_pred):\n",
    "#     mse = mean_squared_error(y_true, y_pred)\n",
    "#     return np.sqrt(mse)\n",
    "\n",
    "# rmse_scorer = make_scorer(root_mean_squared_error, greater_is_better=False)\n",
    "# grid_search = GridSearchCV(estimator=ridge,\n",
    "#                            param_grid=param,\n",
    "#                            cv=5,\n",
    "#                            scoring=rmse_scorer,\n",
    "#                            n_jobs=-1,\n",
    "#                            verbose=2)\n",
    "\n",
    "# grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_estimator = grid_search.best_estimator_\n",
    "# y_train_pred = best_estimator.predict(X_train)\n",
    "# y_test_pred = best_estimator.predict(X_test)\n",
    "# y_unseen_pred = best_estimator.predict(X_unseen)\n",
    "# model_evaluation(y_train, y_train_pred, y_test, y_test_pred, best_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param = {'alpha': np.logspace(-3, 3, 7)}\n",
    "\n",
    "# lasso = Lasso()\n",
    "\n",
    "# def root_mean_squared_error(y_true, y_pred):\n",
    "#     mse = mean_squared_error(y_true, y_pred)\n",
    "#     return np.sqrt(mse)\n",
    "\n",
    "\n",
    "# rmse_scorer = make_scorer(root_mean_squared_error, greater_is_better=False)\n",
    "# grid_search = GridSearchCV(estimator=lasso,\n",
    "#                            param_grid=param,\n",
    "#                            cv=5,\n",
    "#                            scoring=rmse_scorer,\n",
    "#                            n_jobs=-1,\n",
    "#                            verbose=2)\n",
    "\n",
    "# grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/pc/Desktop_linux/chinu/big_mart_sales_prediction/sample_submission_8RXa3c6.csv'\n",
    "submission = pd.read_csv(path)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unseen_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_path = '/home/pc/Desktop_linux/chinu/big_mart_sales_prediction/test_AbJTz2l.csv'\n",
    "unseen_df = pd.read_csv(unseen_path)\n",
    "unseen_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_path = '/home/pc/Desktop_linux/chinu/big_mart_sales_prediction/test_AbJTz2l.csv'\n",
    "unseen_df = pd.read_csv(unseen_path)\n",
    "submission_df = unseen_df[['Item_Identifier', 'Outlet_Identifier']]\n",
    "submission_df['Item_Outlet_Sales'] = y_unseen_pred\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file_path = '/home/pc/Desktop_linux/chinu/big_mart_sales_prediction/submission/1st_attempt.csv'\n",
    "submission_df.to_csv(write_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
